{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import bcolz\n",
    "from sklearn.cross_validation import train_test_split\n",
    "def save_array(fname, arr): c=bcolz.carray(arr, rootdir=fname, mode='w'); c.flush()\n",
    "def load_array(fname): return bcolz.open(fname)[:]\n",
    "cwd = os.getcwd()\n",
    "data_dir = os.path.join(cwd, 'data')\n",
    "im_dir = os.path.join(data_dir, 'train')\n",
    "test_dir = os.path.join(data_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/nbs/invasive_species'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flip Test Picture\n",
    "\n",
    "My model below threw errors when making predictions on the test set, and after some digging I found that one test image is in the wrong orientation. So in the code below I flip it to match all the other images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nbs/invasive_species/data/test\n"
     ]
    }
   ],
   "source": [
    "% cd $test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im = Image.open('1068.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(866, 1154)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rot = im.transpose(Image.ROTATE_270) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1154, 866)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rot.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rot.save('1068.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check = Image.open('1068.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1154, 866)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Train Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nbs/invasive_species/data\n"
     ]
    }
   ],
   "source": [
    "% cd $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the train labels file to split the training images into train and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2295\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>invasive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name  invasive\n",
       "0     1         0\n",
       "1     2         0\n",
       "2     3         1\n",
       "3     4         0\n",
       "4     5         1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv(data_dir + '/train_labels.csv')\n",
    "print(len(labels))\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6309368191721133"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(labels.invasive.sum())\n",
    "1448.0 / 2295.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, 63% of the images are in the positive class. This doesn't seem super imbalanced, but we'll still use a stratified split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr_idx, val_idx = train_test_split(np.arange(len(labels)), stratify=labels.invasive.values, test_size = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1950,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(345,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert 1950 + 345 == len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nbs/invasive_species/data/train\n"
     ]
    }
   ],
   "source": [
    "% cd $im_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make positive and negative image folders for both the train and validation sets, then move the images to the appropriate folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% mkdir tr\n",
    "% mkdir val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% mkdir tr/0\n",
    "% mkdir tr/1\n",
    "% mkdir val/0\n",
    "% mkdir val/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx in tr_idx:\n",
    "    name = str(idx+1) + '.jpg'\n",
    "    label = labels[labels.name == idx+1].invasive\n",
    "    os.rename(name, os.path.join('tr', str(label.values[0]), name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx in val_idx:\n",
    "    name = str(idx+1) + '.jpg'\n",
    "    label = labels[labels.name == idx+1].invasive\n",
    "    os.rename(name, os.path.join('val', str(label.values[0]), name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move test images into a 'wrap' folder to make it easier to load them using pytorch's data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nbs/invasive_species/data/test\n"
     ]
    }
   ],
   "source": [
    "cd $test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% mkdir wrap\n",
    "% mv *.jpg wrap/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that we've moved the images correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '0']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '0']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nbs/invasive_species/data/train/tr/0\n",
      "720\n"
     ]
    }
   ],
   "source": [
    "% cd $im_dir/tr/0\n",
    "g = glob('*.jpg')\n",
    "print(len(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nbs/invasive_species/data/train/tr/1\n",
      "1230\n"
     ]
    }
   ],
   "source": [
    "% cd $im_dir/tr/1\n",
    "g = glob('*.jpg')\n",
    "print(len(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6307692307692307"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1230.0 / (1230 + 720) * 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nbs/invasive_species/data/train/val/0\n",
      "127\n"
     ]
    }
   ],
   "source": [
    "% cd $im_dir/val/0\n",
    "g = glob('*.jpg')\n",
    "print(len(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nbs/invasive_species/data/train/val/1\n",
      "218\n"
     ]
    }
   ],
   "source": [
    "% cd $im_dir/val/1\n",
    "g = glob('*.jpg')\n",
    "print(len(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6318840579710145"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "218.0 / (127 + 218) * 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build Model\n",
    "We're going to finetune a pretrained VGG-16 model, a task for which I found the following references helpful: \n",
    "- https://medium.com/towards-data-science/transfer-learning-using-pytorch-part-2-9c5b18e15551\n",
    "- https://discuss.pytorch.org/t/how-to-perform-finetuning-in-pytorch/419/16\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Precompute the convolutional features\n",
    "First, we'll precompute the output of the convolutional layers of the VGG-16 model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code set ups the infrastructure to load the image data, loads the pretrained model, and makes sure we're running on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG (\n",
       "  (features): Sequential (\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU (inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU (inplace)\n",
       "    (4): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU (inplace)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU (inplace)\n",
       "    (9): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU (inplace)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU (inplace)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU (inplace)\n",
       "    (16): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU (inplace)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU (inplace)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU (inplace)\n",
       "    (23): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU (inplace)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU (inplace)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU (inplace)\n",
       "    (30): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  )\n",
       "  (classifier): Sequential (\n",
       "    (0): Linear (25088 -> 4096)\n",
       "    (1): ReLU (inplace)\n",
       "    (2): Dropout (p = 0.5)\n",
       "    (3): Linear (4096 -> 4096)\n",
       "    (4): ReLU (inplace)\n",
       "    (5): Dropout (p = 0.5)\n",
       "    (6): Linear (4096 -> 1000)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_transform = transforms.Compose([\n",
    "    transforms.Scale((224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "vgg_ds_tr = torchvision.datasets.ImageFolder(root=im_dir + '/tr', transform=vgg_transform)\n",
    "vgg_ds_val = torchvision.datasets.ImageFolder(root=im_dir + '/val', transform=vgg_transform)\n",
    "vgg_ds_test = torchvision.datasets.ImageFolder(root=test_dir, transform=vgg_transform)\n",
    "vgg_tr_loader = torch.utils.data.DataLoader(vgg_ds_tr, batch_size=2, shuffle=False)\n",
    "vgg_val_loader = torch.utils.data.DataLoader(vgg_ds_val, batch_size=2, shuffle=False)\n",
    "vgg_test_loader = torch.utils.data.DataLoader(vgg_ds_test, batch_size=1, shuffle=False)\n",
    "\n",
    "model_vgg = models.vgg16(pretrained=True)\n",
    "\n",
    "for param in model_vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "def preComputeConvFeat(dataset):\n",
    "    conv_features = []\n",
    "    labels_list = []\n",
    "    for data in dataset:\n",
    "        inputs,labels = data\n",
    "        inputs , labels = autograd.Variable(inputs.cuda()),autograd.Variable(labels.cuda())\n",
    "        x = model_vgg.features(inputs)\n",
    "        conv_features.extend(x.data.cpu().numpy())\n",
    "        labels_list.extend(labels.data.cpu().numpy())\n",
    "    conv_features = np.concatenate([[feat] for feat in conv_features])\n",
    "        \n",
    "    return (conv_features,labels_list)\n",
    "\n",
    "\n",
    "model_vgg.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the output of the convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr_conv_feats, tr_labels = preComputeConvFeat(vgg_tr_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1950, 512, 7, 9)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_conv_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nbs/invasive_species\n"
     ]
    }
   ],
   "source": [
    "% cd $cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array('tr_conv_feats.bc', tr_conv_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr_conv_feats = load_array('tr_conv_feats.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_conv_feats, val_labels = preComputeConvFeat(vgg_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(345, 512, 7, 9)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_conv_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array('val_conv_feats.bc', val_conv_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_conv_feats = load_array('val_conv_feats.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1950"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "345"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array('tr_labels.bc', tr_labels)\n",
    "save_array('val_labels.bc', val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr_labels = load_array('tr_labels.bc')\n",
    "val_labels = load_array('val_labels.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_conv_feats, test_labels = preComputeConvFeat(vgg_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1531, 512, 7, 9)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array('test_conv_feats.bc', test_conv_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_conv_feats = load_array('test_conv_feats.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_imgs = [f[0].split('/')[-1] for f in vgg_ds_test.imgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'779.jpg'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(test_imgs))\n",
    "test_imgs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array('test_filenames.bc', test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_imgs = load_array('test_filenames.bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the classifier\n",
    "\n",
    "Now we can use the outputs from the convolutional layers as input to a small fully connected network that can produce the probability of each image containing the invasive species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential (\n",
       "  (0): Linear (32256 -> 4096)\n",
       "  (1): ReLU ()\n",
       "  (2): Dropout2d (p=0.75)\n",
       "  (3): Linear (4096 -> 2)\n",
       "  (4): LogSoftmax ()\n",
       ")"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = nn.Sequential(nn.Linear(32256, 4096), nn.ReLU(), nn.Dropout2d(p=0.75), nn.Linear(4096, 2), nn.LogSoftmax())\n",
    "classifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 32256])\n",
      "True\n",
      "torch.Size([4096])\n",
      "True\n",
      "torch.Size([2, 4096])\n",
      "True\n",
      "torch.Size([2])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for p in classifier.parameters():\n",
    "    print(p.size())\n",
    "    print(p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the input for our model, our loss function and our optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1950, 32256])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_tense = torch.Tensor(tr_conv_feats.reshape(tr_conv_feats.shape[0], -1))\n",
    "tr_tense.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([345, 32256])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_tense = torch.Tensor(val_conv_feats.reshape(val_conv_feats.shape[0], -1))\n",
    "val_tense.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1531, 32256])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tense = torch.Tensor(test_conv_feats.reshape(test_conv_feats.shape[0], -1))\n",
    "test_tense.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1950])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_labels_tense = torch.LongTensor(np.array(tr_labels))\n",
    "tr_labels_tense.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([345])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels_tense = torch.LongTensor(np.array(val_labels))\n",
    "val_labels_tense.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(classifier.parameters(), momentum=0.9, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_feat_tr_ds = torch.utils.data.TensorDataset(tr_tense, tr_labels_tense)\n",
    "conv_feat_tr_loader = torch.utils.data.DataLoader(conv_feat_tr_ds, shuffle=True, batch_size=32)\n",
    "\n",
    "conv_feat_val_ds = torch.utils.data.TensorDataset(val_tense, val_labels_tense)\n",
    "conv_feat_val_loader = torch.utils.data.DataLoader(conv_feat_val_ds, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to evaluate how our model is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def get_accuracy(val_loader):\n",
    "    classifier.eval()\n",
    "    running_loss = 0\n",
    "    num_correct = 0\n",
    "    num_ims = val_loader.dataset.data_tensor.size(0)\n",
    "    for i, data in enumerate(val_loader):    \n",
    "        ims, im_labels = data\n",
    "        ims = autograd.Variable(ims, requires_grad=False)\n",
    "        im_labels = autograd.Variable(im_labels.type(torch.LongTensor), requires_grad=False)\n",
    "        out = classifier(ims)\n",
    "        _, preds = torch.max(out, 1)\n",
    "        num_correct += (preds == im_labels).sum().data[0]\n",
    "    classifier.train()\n",
    "    return ((num_correct * 1.0) / (num_ims * 1.0))\n",
    "     \n",
    "\n",
    "def get_auc(loader, classifier):\n",
    "    classifier.eval()\n",
    "    probs = torch.FloatTensor([])\n",
    "    labels = torch.LongTensor([])\n",
    "    for i, data in enumerate(loader):    \n",
    "        ims, im_labels = data\n",
    "        ims = autograd.Variable(ims, requires_grad=False)\n",
    "        im_labels = autograd.Variable(im_labels.type(torch.LongTensor), requires_grad=False)\n",
    "        out = classifier(ims)\n",
    "        soft = nn.Softmax()\n",
    "        ps  = soft(out)\n",
    "        labels = torch.cat((labels,im_labels.data))\n",
    "        probs = torch.cat((probs, ps.data))\n",
    "    classifier.train()\n",
    "    return roc_auc_score(labels.numpy(), probs.numpy()[:,1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training loop.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy for epoch 0: 0.826666666667\n",
      "validation accuracy for epoch 0: 0.91884057971\n",
      "validation AUC for epoch 0: 0.975077656577\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training accuracy for epoch 1: 0.924102564103\n",
      "validation accuracy for epoch 1: 0.907246376812\n",
      "validation AUC for epoch 1: 0.980603915336\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training accuracy for epoch 2: 0.934871794872\n",
      "validation accuracy for epoch 2: 0.939130434783\n",
      "validation AUC for epoch 2: 0.982987791664\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training accuracy for epoch 3: 0.949230769231\n",
      "validation accuracy for epoch 3: 0.933333333333\n",
      "validation AUC for epoch 3: 0.984179729827\n",
      "--------------------------------------\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 4\n",
    "num_tr_ims = conv_feat_tr_ds.data_tensor.size(0) \n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    running_loss = 0\n",
    "    num_ims_completed = 0\n",
    "    num_correct = 0\n",
    "    for i, data in enumerate(conv_feat_tr_loader):\n",
    "        optimizer.zero_grad()     \n",
    "        ims, im_labels = data\n",
    "        ims = autograd.Variable(ims)\n",
    "        im_labels = autograd.Variable(im_labels.type(torch.LongTensor))\n",
    "        out = classifier(ims)\n",
    "        loss = criterion(out, im_labels)\n",
    "        running_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        num_ims_completed += ims.size()[0]\n",
    "        _, preds = torch.max(out, 1)\n",
    "        num_correct += (preds == im_labels).sum().data[0]\n",
    "   \n",
    "    tr_acc = (num_correct * 1.0) / (num_tr_ims * 1.0)\n",
    "    val_acc = get_accuracy(conv_feat_val_loader)\n",
    "    print('training accuracy for epoch {}: {}'.format(epoch, tr_acc))\n",
    "    print('validation accuracy for epoch {}: {}'.format(epoch, val_acc))\n",
    "    print('validation AUC for epoch {}: {}'.format(epoch, get_auc(conv_feat_val_loader, classifier)))\n",
    "    print('--------------------------------------')\n",
    "    print('--------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(classifier.state_dict(), cwd+ '/invasive_classifier.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Make predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.load_state_dict(torch.load(cwd+ '/invasive_classifier.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nbs/invasive_species/data\n"
     ]
    }
   ],
   "source": [
    "% cd $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samp = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>invasive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name  invasive\n",
       "0     1       0.5\n",
       "1     2       0.5\n",
       "2     3       0.5\n",
       "3     4       0.5\n",
       "4     5       0.5"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_preds(loader, classifier):\n",
    "    classifier.eval()\n",
    "    probs = torch.FloatTensor([])\n",
    "    for i, data in enumerate(loader):    \n",
    "        ims, _ = data\n",
    "        ims = autograd.Variable(ims, requires_grad=False)\n",
    "        out = classifier(ims)\n",
    "        soft = nn.Softmax()\n",
    "        ps  = soft(out)\n",
    "        probs = torch.cat((probs, ps.data))\n",
    "    return probs.numpy()[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_feat_test_ds = torch.utils.data.TensorDataset(test_tense, torch.ones(test_tense.size()[0]))\n",
    "conv_feat_test_loader = torch.utils.data.DataLoader(conv_feat_test_ds, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs = get_preds(conv_feat_test_loader, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1531,)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_data = np.stack((np.array(test_imgs), probs), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1531, 2)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame(sub_data, columns=['name', 'invasive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1531"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>invasive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>779.jpg</td>\n",
       "      <td>0.9951608181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1261.jpg</td>\n",
       "      <td>0.0252922773361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>878.jpg</td>\n",
       "      <td>0.0240965969861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>552.jpg</td>\n",
       "      <td>0.978661596775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>400.jpg</td>\n",
       "      <td>0.368756443262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name         invasive\n",
       "0   779.jpg     0.9951608181\n",
       "1  1261.jpg  0.0252922773361\n",
       "2   878.jpg  0.0240965969861\n",
       "3   552.jpg   0.978661596775\n",
       "4   400.jpg   0.368756443262"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_df['name'] = sub_df.name.apply(lambda x: x.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>invasive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>779</td>\n",
       "      <td>0.9951608181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1261</td>\n",
       "      <td>0.0252922773361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>878</td>\n",
       "      <td>0.0240965969861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>552</td>\n",
       "      <td>0.978661596775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>400</td>\n",
       "      <td>0.368756443262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name         invasive\n",
       "0   779     0.9951608181\n",
       "1  1261  0.0252922773361\n",
       "2   878  0.0240965969861\n",
       "3   552   0.978661596775\n",
       "4   400   0.368756443262"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/nbs/invasive_species\n"
     ]
    }
   ],
   "source": [
    "cd $cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_df.to_csv('invasive_clf_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='invasive_clf_submission.csv' target='_blank'>invasive_clf_submission.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/nbs/invasive_species/invasive_clf_submission.csv"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink('invasive_clf_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
