{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import bcolz\n",
    "from sklearn.cross_validation import train_test_split\n",
    "def save_array(fname, arr): c=bcolz.carray(arr, rootdir=fname, mode='w'); c.flush()\n",
    "def load_array(fname): return bcolz.open(fname)[:]\n",
    "cwd = os.getcwd()\n",
    "data_dir = os.path.join(cwd, 'data')\n",
    "im_dir = os.path.join(data_dir, 'train')\n",
    "test_dir = os.path.join(data_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/courses/deeplearning1/nbs'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flip Test Picture\n",
    "\n",
    "My model below threw errors when making predictions on the test set, and after some digging I found that one test image is in the wrong orientation. So in the code below I flip it to match all the other images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im = Image.open('1068.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(866, 1154)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rot = im.transpose(Image.ROTATE_270) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1154, 866)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rot.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rot.save('1068.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check = Image.open('1068.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1154, 866)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Train Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = pd.read_csv(data_dir + '/train_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2295"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>invasive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name  invasive\n",
       "0     1         0\n",
       "1     2         0\n",
       "2     3         1\n",
       "3     4         0\n",
       "4     5         1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1448"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.invasive.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6309368191721133"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1448.0 / 2295.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get indices to split the training images into a train and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr_idx, val_idx = train_test_split(np.arange(len(labels)), stratify=labels.invasive.values, test_size = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1950,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(345,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert 1950 + 345 == len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/courses/deeplearning1/nbs/data/train\n"
     ]
    }
   ],
   "source": [
    "% cd $im_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make positive and negative image folders for both the train and validation sets, then move the images to the appropriate folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% mkdir tr\n",
    "% mkdir val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% mkdir tr/0\n",
    "% mkdir tr/1\n",
    "% mkdir val/0\n",
    "% mkdir val/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx in tr_idx:\n",
    "    name = str(idx+1) + '.jpg'\n",
    "    label = labels[labels.name == idx+1].invasive\n",
    "    os.rename(name, os.path.join('tr', str(label.values[0]), name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx in val_idx:\n",
    "    name = str(idx+1) + '.jpg'\n",
    "    label = labels[labels.name == idx+1].invasive\n",
    "    os.rename(name, os.path.join('val', str(label.values[0]), name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that we've moved the images correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '0']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '0']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/courses/deeplearning1/nbs/data/train/tr/0\n",
      "720\n"
     ]
    }
   ],
   "source": [
    "% cd $im_dir/tr/0\n",
    "g = glob('*.jpg')\n",
    "print(len(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/courses/deeplearning1/nbs/data/train/tr/1\n",
      "1230\n"
     ]
    }
   ],
   "source": [
    "% cd $im_dir/tr/1\n",
    "g = glob('*.jpg')\n",
    "print(len(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6307692307692307"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1230.0 / (1230 + 720) * 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ryanryanadmin/Documents/ds/invasive/input/train/val/0\n",
      "127\n"
     ]
    }
   ],
   "source": [
    "% cd $im_dir/val/0\n",
    "g = glob('*.jpg')\n",
    "print(len(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/courses/deeplearning1/nbs/data/train/val/1\n",
      "218\n"
     ]
    }
   ],
   "source": [
    "% cd $im_dir/val/1\n",
    "g = glob('*.jpg')\n",
    "print(len(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6318840579710145"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "218.0 / (127 + 218) * 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build Model\n",
    "We're going to finetune a pretrained VGG-16 model, a task for which I found the following references helpful: \n",
    "- https://medium.com/towards-data-science/transfer-learning-using-pytorch-part-2-9c5b18e15551\n",
    "- https://discuss.pytorch.org/t/how-to-perform-finetuning-in-pytorch/419/16\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Precompute the convolutional features\n",
    "First, we'll precompute the output of the convolutional layers of the VGG-16 model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code set ups the infrastructure to load the image data, loads the pretrained model, and makes sure we're running on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg_transform = transforms.Compose([\n",
    "    transforms.Scale((224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "vgg_ds_tr = torchvision.datasets.ImageFolder(root=im_dir + '/tr', transform=vgg_transform)\n",
    "\n",
    "vgg_ds_val = torchvision.datasets.ImageFolder(root=im_dir + '/val', transform=vgg_transform)\n",
    "\n",
    "vgg_ds_test = torchvision.datasets.ImageFolder(root=test_dir, transform=vgg_transform)\n",
    "\n",
    "vgg_tr_loader = torch.utils.data.DataLoader(vgg_ds_tr, batch_size=2, shuffle=False)\n",
    "\n",
    "vgg_val_loader = torch.utils.data.DataLoader(vgg_ds_val, batch_size=2, shuffle=False)\n",
    "\n",
    "vgg_test_loader = torch.utils.data.DataLoader(vgg_ds_test, batch_size=1, shuffle=False)\n",
    "\n",
    "model_vgg = models.vgg16(pretrained=True)\n",
    "\n",
    "for param in model_vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "def preconvfeat(dataset):\n",
    "    conv_features = []\n",
    "    labels_list = []\n",
    "    for data in dataset:\n",
    "        inputs,labels = data\n",
    "        inputs , labels = autograd.Variable(inputs.cuda()),autograd.Variable(labels.cuda())\n",
    "        x = model_vgg.features(inputs)\n",
    "        conv_features.extend(x.data.cpu().numpy())\n",
    "        labels_list.extend(labels.data.cpu().numpy())\n",
    "    conv_features = np.concatenate([[feat] for feat in conv_features])\n",
    "        \n",
    "    return (conv_features,labels_list)\n",
    "\n",
    "\n",
    "model_vgg.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the output of the convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr_conv_feats, tr_labels = preconvfeat(vgg_tr_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1950, 512, 7, 9)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_conv_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/courses/deeplearning1/nbs\n"
     ]
    }
   ],
   "source": [
    "% cd $cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array('tr_conv_feats.bc', tr_conv_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr_conv_feats = load_array('tr_conv_feats.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_conv_feats, val_labels = preconvfeat(vgg_val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(345, 512, 7, 9)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_conv_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array('val_conv_feats.bc', val_conv_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_conv_feats = load_array('val_conv_feats.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1950"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "345"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array('tr_labels.bc', tr_labels)\n",
    "save_array('val_labels.bc', val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr_labels = load_array('tr_labels.bc')\n",
    "val_labels = load_array('val_labels.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_conv_feats, test_labels = preconvfeat(vgg_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1531, 512, 7, 9)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_conv_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array('test_conv_feats.bc', test_conv_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_conv_feats = load_array('test_conv_feats.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_imgs = [f[0].split('/')[-1] for f in vgg_ds_test.imgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'779.jpg'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(test_imgs))\n",
    "test_imgs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array('test_filenames.bc', test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_imgs = load_array('test_filenames.bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the classifier\n",
    "\n",
    "Now we can use the outputs from the convolutional layers as input to small network that can produce the probability of each image containing the invasive species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = nn.Sequential(nn.Linear(32256, 4096), nn.ReLU(), nn.Dropout2d(p=0.5), nn.Linear(4096, 2), nn.LogSoftmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Sequential.state_dict of Sequential (\n",
       "  (0): Linear (32256 -> 4096)\n",
       "  (1): ReLU ()\n",
       "  (2): Dropout2d (p=0.5)\n",
       "  (3): Linear (4096 -> 2)\n",
       "  (4): LogSoftmax ()\n",
       ")>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 32256])\n",
      "True\n",
      "torch.Size([4096])\n",
      "True\n",
      "torch.Size([2, 4096])\n",
      "True\n",
      "torch.Size([2])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for p in classifier.parameters():\n",
    "    print(p.size())\n",
    "    print(p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1950, 32256])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_tense = torch.Tensor(tr_conv_feats.reshape(tr_conv_feats.shape[0], -1))\n",
    "tr_tense.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([345, 32256])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_tense = torch.Tensor(val_conv_feats.reshape(val_conv_feats.shape[0], -1))\n",
    "val_tense.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1531, 32256])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tense = torch.Tensor(test_conv_feats.reshape(test_conv_feats.shape[0], -1))\n",
    "test_tense.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1950])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_labels_tense = torch.LongTensor(np.array(tr_labels))\n",
    "tr_labels_tense.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([345])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels_tense = torch.LongTensor(np.array(val_labels))\n",
    "val_labels_tense.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(classifier.parameters(), momentum=0.9, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_feat_tr_ds = torch.utils.data.TensorDataset(tr_tense, tr_labels_tense)\n",
    "conv_feat_tr_loader = torch.utils.data.DataLoader(conv_feat_tr_ds, shuffle=True, batch_size=32)\n",
    "\n",
    "conv_feat_val_ds = torch.utils.data.TensorDataset(val_tense, val_labels_tense)\n",
    "conv_feat_val_loader = torch.utils.data.DataLoader(conv_feat_val_ds, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_accuracy(val_loader):\n",
    "    running_loss = 0\n",
    "    num_correct = 0\n",
    "    num_ims = val_loader.dataset.data_tensor.size(0)\n",
    "    for i, data in enumerate(val_loader):    \n",
    "        ims, im_labels = data\n",
    "        ims = autograd.Variable(ims, requires_grad=False)\n",
    "        im_labels = autograd.Variable(im_labels.type(torch.LongTensor), requires_grad=False)\n",
    "        out = classifier(ims)\n",
    "        _, preds = torch.max(out, 1)\n",
    "        num_correct += (preds == im_labels).sum().data[0]\n",
    "     \n",
    "    return ((num_correct * 1.0) / (num_ims * 1.0))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def get_auc(loader, classifier):\n",
    "    probs = torch.FloatTensor([])\n",
    "    labels = torch.LongTensor([])\n",
    "    for i, data in enumerate(loader):    \n",
    "        ims, im_labels = data\n",
    "        ims = autograd.Variable(ims, requires_grad=False)\n",
    "        im_labels = autograd.Variable(im_labels.type(torch.LongTensor), requires_grad=False)\n",
    "        out = classifier(ims)\n",
    "        soft = nn.Softmax()\n",
    "        ps  = soft(out)\n",
    "        labels = torch.cat((labels,im_labels.data))\n",
    "        probs = torch.cat((probs, ps.data))\n",
    "    return roc_auc_score(labels.numpy(), probs.numpy()[:,1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy for epoch 0: 0.821538461538\n",
      "validation accuracy for epoch 0: 0.915942028986\n",
      "validation AUC for epoch 0: 0.972224228852\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training accuracy for epoch 1: 0.932307692308\n",
      "validation accuracy for epoch 1: 0.936231884058\n",
      "validation AUC for epoch 1: 0.983132269017\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "training accuracy for epoch 2: 0.950256410256\n",
      "validation accuracy for epoch 2: 0.942028985507\n",
      "validation AUC for epoch 2: 0.982301524236\n",
      "--------------------------------------\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 3\n",
    "num_tr_ims = conv_feat_tr_ds.data_tensor.size(0) \n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    running_loss = 0\n",
    "    num_ims_completed = 0\n",
    "    num_correct = 0\n",
    "    for i, data in enumerate(conv_feat_tr_loader):\n",
    "        optimizer.zero_grad()     \n",
    "        ims, im_labels = data\n",
    "        ims = autograd.Variable(ims)\n",
    "        im_labels = autograd.Variable(im_labels.type(torch.LongTensor))\n",
    "        out = classifier(ims)\n",
    "        loss = criterion(out, im_labels)\n",
    "        running_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        num_ims_completed += ims.size()[0]\n",
    "        _, preds = torch.max(out, 1)\n",
    "        num_correct += (preds == im_labels).sum().data[0]\n",
    "   \n",
    "    tr_acc = (num_correct * 1.0) / (num_tr_ims * 1.0)\n",
    "    val_acc = get_accuracy(conv_feat_val_loader)\n",
    "    print('training accuracy for epoch {}: {}'.format(epoch, tr_acc))\n",
    "    print('validation accuracy for epoch {}: {}'.format(epoch, val_acc))\n",
    "    print('validation AUC for epoch {}: {}'.format(epoch, get_auc(conv_feat_val_loader, classifier)))\n",
    "    print('--------------------------------------')\n",
    "    print('--------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(classifier.state_dict(), cwd+ '/invasive_classifier_2.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Make predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier.load_state_dict(torch.load(cwd+ '/invasive_classifier_2.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/courses/deeplearning1/nbs/data\n"
     ]
    }
   ],
   "source": [
    "% cd $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samp = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>invasive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name  invasive\n",
       "0     1       0.5\n",
       "1     2       0.5\n",
       "2     3       0.5\n",
       "3     4       0.5\n",
       "4     5       0.5"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_preds(loader, classifier):\n",
    "    probs = torch.FloatTensor([])\n",
    "    for i, data in enumerate(loader):    \n",
    "        ims, _ = data\n",
    "        ims = autograd.Variable(ims, requires_grad=False)\n",
    "        out = classifier(ims)\n",
    "        soft = nn.Softmax()\n",
    "        ps  = soft(out)\n",
    "        probs = torch.cat((probs, ps.data))\n",
    "    return probs.numpy()[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_feat_test_ds = torch.utils.data.TensorDataset(test_tense, torch.ones(test_tense.size()[0]))\n",
    "conv_feat_test_loader = torch.utils.data.DataLoader(conv_feat_test_ds, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs = get_preds(conv_feat_test_loader, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1531,)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_data = np.stack((np.array(test_imgs), probs), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1531, 2)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame(sub_data, columns=['name', 'invasive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1531"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>invasive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>779.jpg</td>\n",
       "      <td>0.981414496899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1261.jpg</td>\n",
       "      <td>0.0294399186969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>878.jpg</td>\n",
       "      <td>0.0701650455594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>552.jpg</td>\n",
       "      <td>0.939455389977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>400.jpg</td>\n",
       "      <td>0.0528514869511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name         invasive\n",
       "0   779.jpg   0.981414496899\n",
       "1  1261.jpg  0.0294399186969\n",
       "2   878.jpg  0.0701650455594\n",
       "3   552.jpg   0.939455389977\n",
       "4   400.jpg  0.0528514869511"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_df['name'] = sub_df.name.apply(lambda x: x.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>invasive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>779</td>\n",
       "      <td>0.981414496899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1261</td>\n",
       "      <td>0.0294399186969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>878</td>\n",
       "      <td>0.0701650455594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>552</td>\n",
       "      <td>0.939455389977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>400</td>\n",
       "      <td>0.0528514869511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name         invasive\n",
       "0   779   0.981414496899\n",
       "1  1261  0.0294399186969\n",
       "2   878  0.0701650455594\n",
       "3   552   0.939455389977\n",
       "4   400  0.0528514869511"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/courses/deeplearning1/nbs\n"
     ]
    }
   ],
   "source": [
    "cd $cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_df.to_csv('invasive_clf_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='invasive_clf_submission.csv' target='_blank'>invasive_clf_submission.csv</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/courses/deeplearning1/nbs/invasive_clf_submission.csv"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink('invasive_clf_submission.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
